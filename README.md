# üß† SOKE Graph: A Semantic-linked Ontological Framework for Domain-Specific Knowledge Discovery in Scientific Literature

SOKE Graph is a powerful, end-to-end pipeline designed to extract structured knowledge from scientific PDFs using ontology-driven classification and AI-assisted language models. It enables automated discovery and categorisation of domain-specific information‚Äîsuch as catalyst types, reaction conditions, and performance metrics‚Äîby parsing research papers, classifying concepts across multiple layers (e.g., Process, Environment, Reaction), and storing the results in a knowledge graph.

This tool can be tailored for accelerating literature analysis in any domain of research; in our case, we have focused on material science fields like green hydrogen production and water electrolysis.

## üìë Table of Contents

- [üß† SOKE Graph: A Semantic-linked Ontological Framework](#-soke-graph-a-semantic-linked-ontological-framework-for-domain-specific-knowledge-discovery-in-scientific-literature)
- [üöÄ Features](#-features)
- [üöÄ How to Run This Python Project on Windows, macOS, and Linux](#-how-to-run-this-python-project-on-windows-macos-and-linux)
  - [Step 1: Open the Command Line / Terminal](#step-1-open-the-command-line--terminal)
  - [Step 2: Clone the Project (Download the Code)](#step-2-clone-the-project-download-the-code)
  - [Step 3: Create a Virtual Environment](#step-3-create-a-virtual-environment-conda-recommended)
  - [Step 4: Activate the Virtual Environment](#step-4-activate-the-environment)
  - [Step 5: Install Project Dependencies](#step-5-install-project-dependencies)
  - [Step 6: Recommended Editor ‚Äì VS Code](#step-6-recommended-editor--visual-studio-code-vs-code)
  - [Step 7: Run the Project](#step-7-run-the-project)
    - [1Ô∏è‚É£ Run with Streamlit App](#1Ô∏è‚É£-run-with-streamlit-app--streamlit-apppy)
    - [2Ô∏è‚É£ Run from Jupyter Notebook ‚Äì full_pipeline.ipynb](#2Ô∏è‚É£-run-from-jupyter-notebook--full_pipelineipynb)
    - [3Ô∏è‚É£ Run from Jupyter Notebook ‚Äì Step-by-Step](#3Ô∏è‚É£-run-from-jupyter-notebook-interactive-step-by-step--full_pipeline_stepbystepipynb)
- [üìÇ Preparing Input Files for SOKEGraph](#-preparing-input-files-for-sokegraph)
  - [1) Ontology File](#1-ontology-file-ontologyjson)
  - [2) Query File](#2-query-file-paper_querytxt)
  - [3) Keyword File](#3-keyword-file-keyword_querytxt)
- [Step 8: Deactivate Virtual Environment](#step-8-deactivate-virtual-environment-optional)
- [Reusing the Tool](#reusing-the-tool)

## üöÄ Features

- üîç **Retrieve papers** from Semantic Scholar or your PDF collection
- ü§ñ **Use AI (OpenAI, Gemini, ...)** to extract ontological concepts and metadata
- üìä **Rank papers** based on query relevance and extracted metadata
- üß± **Build knowledge graphs** (Neo4j supported) from structured paper data

---
# üöÄ How to Run This Python Project on Windows, macOS, and Linux

This guide will walk you through running this project on your computer, regardless of your operating system or prior Python knowledge.

---

## Step 1: Open the Command Line / Terminal

You'll be able to enter commands here.

- **Windows:** Press `Win + R`, type `cmd`, and press Enter to open Command Prompt.  
  Or, press `Win + X`, then select **Windows PowerShell** or **Windows Terminal** if installed.

- **macOS:** Press `Cmd + Space`, type `Terminal`, and press Enter.

- **Linux:** Look for the Terminal app in your applications menu, or press `Ctrl + Alt + T`.

---

## Step 2: Clone the Project (Download the Code)

In the command line window you opened, type:

```bash
git clone https://github.com/sokematgraph/SOKE-Graph.git
```

üëâ If Git is not installed on your system, please see [INSTALLATION.md](INSTALLATION.md) for details.

After cloning, navigate into the project folder:
  ```bash
  cd SOKEGraph
  ```

## Step 3: Create a Virtual Environment (Conda Recommended)

We recommend using **Conda** (or Miniconda/Mamba) to manage dependencies for this project.  
Conda makes it easy to install and manage scientific packages across platforms.

```bash
conda create -n sokegraph python=3.9
```

---

## Step 4: Activate the Environment

```bash
conda activate sokegraph
```

You‚Äôll need to activate this environment every time before running the project.

---
## Step 5: Install Project Dependencies

With the environment active, install required packages:

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

> ‚ö° **Note for advanced users:**  
> You may also use Python‚Äôs built-in `venv` if you prefer, but Conda is the recommended and tested way for this project.

---


## Step 6: Recommended Editor ‚Äì Visual Studio Code (VS Code)

We recommend using **Visual Studio Code (VS Code)** for working with this project, whether you want to edit code, run the Streamlit app, or work in Jupyter Notebooks.

### Installing VS Code
If you don‚Äôt already have VS Code installed, please see [INSTALLATION.md](INSTALLATION.md) for detailed instructions on how to download and install it.

### Opening the Project
You can open the `SOKEGraph` project folder in two ways:

- **Option 1: From VS Code directly**  
  - Open **VS Code**  
  - Go to **File > Open Folder...** and select the `SOKEGraph` folder  

- **Option 2: From the Terminal**  
  If VS Code is installed and added to your PATH, you can run:  
  ```bash
  cd SOKEGraph
  code .
  ```

After opening, use the integrated terminal (**View > Terminal**) to activate your virtual environment (see Step 4) and start running the project.

### Install VS Code Extensions
- Python (Microsoft)  
- Jupyter (Microsoft)  

These extensions make it easier to run and edit `.py` or `.ipynb` files directly inside VS Code.

üí° Tip: You can run Jupyter notebooks inside VS Code without opening a separate browser window.


## Step 7: Run the Project
You can choose the method that best fits your skills and setup. For most users, **Streamlit app is the easiest way** to get started.

---


### 1Ô∏è‚É£ Run with Streamlit App ‚Äì `streamlit-app.py`

The Streamlit app provides a **simple graphical interface** to run the entire pipeline without writing code.  

#### How to start the app
From your project folder, run:

```bash
streamlit run streamlit-app.py
```

#### What you‚Äôll see
The app will open in your browser. You can configure the pipeline with the following inputs:

- **Paper source**: Choose how to retrieve papers  
  - `Semantic Scholar`  
  - `Journal API`  
  - `PDF ZIP` (upload a ZIP file of papers)  

- **Number of papers**: The maximum number of papers to fetch (for Semantic Scholar or Journal API).  

- **Upload query file (`paper_query.txt`)**: A text file with one search query per line.  

- **Upload base ontology file (`Ontology.json`)**: Defines categories, subcategories, and keywords for concept detection.  

- **Field of interest**: Enter your research domain (e.g., *materials science, biology, medicine*).  

- **AI Agent**: Select which LLM to use for ontology enrichment and paper analysis (`openAI`, `gemini`, `llama`, `ollama`, or `claude`).  

- **Upload API key file (`apikeys_xxx.txt`)**: Required for accessing AI models and/or Journal APIs.  

- **Upload keywords file (`keyword_query.txt`)**: A list of keywords used for ranking and filtering papers.  

- **Knowledge Graph backend**: Choose the graph engine:  
  - `networkx` (in-memory, default)  
  - `neo4j` (requires credentials file)  

- **Compute Device**:  
  By default, the program runs with **GPU acceleration** if your system supports it (e.g., CUDA, MPS).  
  - To force the program to run on **CPU only**, check the option **‚ÄúForce CPU (ignore GPU)‚Äù** in the sidebar.  

#### Running the pipeline
Once all inputs are set, click **üöÄ Run Pipeline**.  

The app will:  
1. Fetch papers from your chosen source.  
2. Enrich the ontology with AI.  
3. Rank the papers using queries and keywords.  
4. Build and display the knowledge graph.  
5. Export results (ranked papers, ontology, graph data) into the `output/` folder.  


### 2Ô∏è‚É£ Run from Jupyter Notebook ‚Äì `full_pipeline.ipynb`  
This notebook is designed for users who are comfortable modifying code directly.

- üîß You should define all parameters manually in a Python dictionary called `params`.
- ‚úÖ Once configured, you run the pipeline **with a single function call**.
- üìÇ Best for quick experiments or automation in notebook environments.

#### Example Usage:

```python
from types import SimpleNamespace
from sokegraph.full_pipeline import full_pipeline_main

params = SimpleNamespace(
    paper_source="Semantic Scholar",  # Options: "Semantic Scholar", "PDF Zip", "Journal API"
    number_papers=10,                # Number of papers to fetch from Semantic Scholar
    paper_query_file="topics.txt",   # Text file with one search query per line
    pdfs_file=None,                  # Optional: ZIP file with PDFs (for PDF source)
    api_key_file="api_journal_api.txt",  # API key file for Journal API source
    ontology_file="base_ontology.json",  # Base ontology file (JSON or OWL)
    AI="openAI",                          # Options: "openAI", "gemini", "llama", "ollama", "claude"
    API_keys="openai_keys.json",         # API key file for AI tools
    keyword_query_file="keywords.txt",   # Text file listing keywords
    model_knowledge_graph="neo4j",       # Options: "neo4j", "networkx"
    credentials_for_knowledge_graph="neo4j_credentials.json",  # Graph DB credentials
    output_dir="output/"                 # Output directory
)

full_pipeline_main(params)
```

‚ö†Ô∏è Important: You should use either:

"number_papers" + "paper_query_file"

OR

"number_papers" + "paper_query_file" + "api_key_journal_api"

OR

"pdfs_file"

depending on whether you're searching for papers or uploading PDFs.

üí° Make sure that all file paths in your `params` are valid and that services like **Neo4j**, **Ollama**, or your Journal API access are available before starting the pipeline.


### 3Ô∏è‚É£ Run from Jupyter Notebook (Interactive Step-by-Step) ‚Äî `full_pipeline_stepBYstep.ipynb`

This notebook uses **ipywidgets** to provide an **interactive form-like interface** for running the pipeline.  
It‚Äôs helpful if you want a guided, cell-by-cell execution **without writing code manually**.

#### üß© What it does:
- Allows you to select how you want to retrieve papers:
  - üìÅ Upload a ZIP file of PDFs (PDF source)
  - üîé Search and fetch papers from **Semantic Scholar** using a query file
  - üåê Fetch papers via the **Journal API** using a query file and an API key

- Provides dropdowns and file pickers to easily select files like:
  - Ontology
  - Keyword queries
  - API keys
  - Output folder

- Runs each pipeline step independently, so you can see exactly what happens at every stage.

#### üìã Steps Involved:

1. **üìÑ Paper Retrieval**
   - Based on your selected `paper_source`:
     - `Semantic Scholar`: Downloads papers using your `paper_query_file`
     - `PDF Zip`: Loads and processes PDFs from the uploaded ZIP file
     - `Journal API`: Retrieves paper metadata from the Web of Science API using query + API key

2. **üß† Ontology Enrichment**
   - The chosen AI agent (`openAI`, `gemini`, `llama`, `ollama`, or `claude`) analyzes the papers and expands your base ontology
   - Adds new keywords, concepts, synonyms, and relationships

3. **üìä Paper Ranking**
   - Ranks the papers using:
     - Exact keyword matches
     - Synonyms and expanded terms
     - Opposite-term filtering to down-rank irrelevant papers

4. **üï∏ Knowledge Graph Construction**
   - Converts enriched data into a structured graph using:
     - `Neo4j` (with login credentials)
     - Or `NetworkX` (in-memory option)
   - Graph includes:
     - Ontology categories
     - Paper-concept links
     - Metadata associations

5. **üíæ Output**
   - Saves everything in your selected `output_dir`, including:
     - Enriched ontology file
     - Ranked papers (CSV/JSON)

---

> ‚úÖ **No need to modify code manually** ‚Äì just fill out the form and click **Run** for each step.

> üí° Make sure required services like **Neo4j**, **Ollama**, or your **Journal API** credentials are ready before starting the pipeline.
  
---



---

# üìÇ Preparing Input Files for SOKEGraph

SOKEGraph uses three input files. Place them in your project (e.g., `./inputs/`) and point the app/notebook to their paths.

## 1) üß≠ Ontology File (`Ontology.json`)
Defines **categories ‚Üí subcategories ‚Üí keywords/synonyms** that guide concept detection and search.

**Format**
```json
{
  "Category": {
    "Subcategory": ["keyword1", "keyword2", "keyword3"]
  }
}
```

**Example**
```json
{
  "Environment": {
    "Acidic": ["pH < 7", "acidic"],
    "Alkaline": ["pH > 7", "alkaline", "basic"]
  },
  "Process": {
    "Water Electrolysis": ["electrolysis of water", "splitting H2O"],
    "Fuel Cells": ["fuel cell", "PEM", "proton exchange membrane"]
  }
}
```

**Tips**
- Include common variants (symbols, abbreviations, spacing: `pH<7` vs `pH < 7`).
- Validate JSON (e.g., jsonlint). Save as `Ontology.json`.

## 2) üìÑ Query File (`paper_query.txt`)
Each **line** is one search query sent to Semantic Scholar / other engines.

**Example**
```txt
Acidic earth abundant catalysts for water splitting
Nickel-based electrocatalysts for OER
Graph neural networks for chemical reaction prediction
```

## 3) üîë Keyword File (`keyword_query.txt`)
The `keyword_query.txt` file contains keywords or short phrases (e.g., *acidic HER water splitting*) that the system uses to **rank papers** during search.

**Example**
```txt
acidic HER water splitting
```

**Recommended Layout**
```
inputs/
  Ontology.json
  paper_query.txt
  keyword_query.txt
```
Point the Streamlit app / notebooks to these files when prompted.


## Step 8: Deactivate Virtual Environment (Optional)
When done working, you can leave the environment by running:
```bash
deactivate
```

Or for Conda:
```bash
conda deactivate
```

---
## Reusing the Tool
If you have cloned the repository and want to use it again:

### Windows Users

In Command Prompt or PowerShell, run:

```bash
cd SOKEGraph
venv\Scripts\activate
streamlit run streamlit-app.py
```